{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfFiles=0\n",
    "try: \n",
    "    filesXtrain = os.listdir('../TrainingDataset/x_train/'); \n",
    "    numberOfFiles = len(filesXtrain)\n",
    "except: print('File not found')\n",
    "numberOfFiles-=11\n",
    "testingSetSize = 100\n",
    "y2_train = np.empty((numberOfFiles-testingSetSize,24,144))\n",
    "y2_test = np.empty((testingSetSize,24,144))\n",
    "\n",
    "for i in range(0,numberOfFiles-testingSetSize):\n",
    "    y2_train[i] = np.load('../TrainingDataset/x_train/Y2_36_60_{}.npy'.format(i))\n",
    "for i in range(numberOfFiles-testingSetSize,numberOfFiles):\n",
    "    y2_test[i-(numberOfFiles-testingSetSize)-1] = np.load('../TrainingDataset/x_train/Y2_36_60_{}.npy'.format(i))\n",
    "y2_train = y2_train.reshape(2899,24,144,1)\n",
    "y2_test = y2_test.reshape(100,24,144,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Encoder\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Decoder\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "    masked_img = Input(shape=img_shape)\n",
    "    gen_missing = model(masked_img)\n",
    "    return Model(masked_img, gen_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=missing_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.summary()\n",
    "\n",
    "    img = Input(shape=missing_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 24\n",
    "img_cols = 144\n",
    "mask_height = 12\n",
    "mask_width = 72\n",
    "channels = 1\n",
    "num_classes = 2\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "missing_shape = (mask_height, mask_width, channels)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates the missing\n",
    "# part of the image\n",
    "masked_img = Input(shape=img_shape)\n",
    "gen_missing = generator(masked_img)\n",
    "\n",
    "# # For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# # The discriminator takes generated images as input and determines\n",
    "# # if it is generated or if it is a real image\n",
    "valid = discriminator(gen_missing)\n",
    "\n",
    "# # # The combined model  (stacked generator and discriminator)\n",
    "# # # Trains generator to fool discriminator\n",
    "combined = Model(masked_img , [gen_missing, valid])\n",
    "combined.compile(loss='mse',\n",
    "    loss_weights=[0.999, 0.001],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_randomly(imgs):\n",
    "    y1 = np.random.randint(0, img_rows - mask_height, imgs.shape[0])\n",
    "    y2 = y1 + mask_height\n",
    "    x1 = np.random.randint(0, img_cols - mask_width, imgs.shape[0])\n",
    "    x2 = x1 + mask_width\n",
    "\n",
    "    masked_imgs = np.empty_like(imgs)\n",
    "    missing_parts = np.empty((imgs.shape[0], mask_height, mask_width, channels))\n",
    "    for i, img in enumerate(imgs):\n",
    "        masked_img = img.copy()\n",
    "        _y1, _y2, _x1, _x2 = y1[i], y2[i], x1[i], x2[i]\n",
    "        missing_parts[i] = masked_img[_y1:_y2, _x1:_x2, :].copy()\n",
    "        masked_img[_y1:_y2, _x1:_x2, :] = 0\n",
    "        masked_imgs[i] = masked_img\n",
    "\n",
    "    return masked_imgs, missing_parts, (y1, y2, x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "X_train = y2_train.copy()\n",
    "def train(epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        masked_imgs, missing_parts, _ = mask_randomly(imgs)\n",
    "        if not math.isnan(np.sum(imgs)) and not math.isnan(np.sum(masked_imgs)):\n",
    "            # Generate a batch of new images\n",
    "            gen_missing = generator.predict(masked_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(missing_parts, valid)\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_missing, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = combined.train_on_batch(masked_imgs, [missing_parts, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "            if epoch % sample_interval == 0:\n",
    "                idx = np.random.randint(0, X_train.shape[0], 6)\n",
    "                imgs = X_train[idx]\n",
    "                sample_images(epoch, imgs)\n",
    "        else:\n",
    "            print(\"nan value in training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, imgs):\n",
    "    r, c = 3, 6\n",
    "\n",
    "    masked_imgs, missing_parts, (y1, y2, x1, x2) = mask_randomly(imgs)\n",
    "    gen_missing = generator.predict(masked_imgs)\n",
    "\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "    masked_imgs = 0.5 * masked_imgs + 0.5\n",
    "    gen_missing = 0.5 * gen_missing + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.1)\n",
    "    fig.set_size_inches(20,3)\n",
    "    for i in range(c):\n",
    "        axs[0,i].imshow(imgs[i, :,:].reshape(24,144),cmap=plt.get_cmap('jet', 20))\n",
    "        axs[0,i].axis('off')\n",
    "        axs[1,i].imshow(masked_imgs[i, :,:].reshape(24, 144),cmap=plt.get_cmap('jet', 20))\n",
    "        axs[1,i].axis('off')\n",
    "        filled_in = imgs[i].copy()\n",
    "        filled_in[y1[i]:y2[i], x1[i]:x2[i], :] = gen_missing[i]\n",
    "        axs[2,i].imshow(filled_in.reshape(24, 144),cmap=plt.get_cmap('jet', 20))\n",
    "        axs[2,i].axis('off')\n",
    "    fig.savefig(\"images_CEGAN/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "\n",
    "    def save(model, model_name):\n",
    "        model_path = \"saved_model_CEGAN/%s.json\" % model_name\n",
    "        weights_path = \"saved_model_CEGAN/%s_weights.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path,\n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "\n",
    "    save(generator, \"generator\")\n",
    "    save(discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.572978, acc: 40.62%] [G loss: 0.331609, mse: 0.074532]\n",
      "1 [D loss: 0.498551, acc: 49.61%] [G loss: 0.325288, mse: 0.065134]\n",
      "2 [D loss: 0.488920, acc: 50.00%] [G loss: 0.323375, mse: 0.071787]\n",
      "nan value in training set\n",
      "nan value in training set\n",
      "5 [D loss: 0.116947, acc: 80.47%] [G loss: 0.392156, mse: 0.074892]\n",
      "nan value in training set\n",
      "7 [D loss: 0.054401, acc: 91.80%] [G loss: 0.386538, mse: 0.087274]\n",
      "8 [D loss: 0.085901, acc: 85.55%] [G loss: 0.173470, mse: 0.087414]\n",
      "9 [D loss: 0.000812, acc: 100.00%] [G loss: 0.106544, mse: 0.091897]\n",
      "nan value in training set\n",
      "11 [D loss: 0.000614, acc: 100.00%] [G loss: 0.085937, mse: 0.082824]\n",
      "nan value in training set\n",
      "13 [D loss: 0.004205, acc: 99.61%] [G loss: 0.066825, mse: 0.065856]\n",
      "14 [D loss: 0.005098, acc: 100.00%] [G loss: 0.065048, mse: 0.061973]\n",
      "15 [D loss: 0.000248, acc: 100.00%] [G loss: 0.053196, mse: 0.050397]\n",
      "16 [D loss: 0.000133, acc: 100.00%] [G loss: 0.050705, mse: 0.050268]\n",
      "17 [D loss: 0.000248, acc: 100.00%] [G loss: 0.044391, mse: 0.042788]\n",
      "18 [D loss: 0.001135, acc: 100.00%] [G loss: 0.042493, mse: 0.039706]\n",
      "nan value in training set\n",
      "20 [D loss: 0.003971, acc: 100.00%] [G loss: 0.045146, mse: 0.043767]\n",
      "21 [D loss: 0.002500, acc: 99.61%] [G loss: 0.048633, mse: 0.043196]\n",
      "nan value in training set\n",
      "23 [D loss: 0.000975, acc: 100.00%] [G loss: 0.041517, mse: 0.041332]\n",
      "24 [D loss: 0.001078, acc: 100.00%] [G loss: 0.043474, mse: 0.041141]\n",
      "25 [D loss: 0.014090, acc: 98.44%] [G loss: 0.091390, mse: 0.043499]\n",
      "26 [D loss: 0.148627, acc: 77.34%] [G loss: 1.056021, mse: 0.057394]\n",
      "27 [D loss: 0.488587, acc: 50.00%] [G loss: 1.068192, mse: 0.068239]\n",
      "28 [D loss: 0.494282, acc: 50.00%] [G loss: 1.056046, mse: 0.057786]\n",
      "29 [D loss: 0.490577, acc: 50.00%] [G loss: 1.046548, mse: 0.055881]\n",
      "30 [D loss: 0.476419, acc: 51.56%] [G loss: 1.013677, mse: 0.045345]\n",
      "31 [D loss: 0.512504, acc: 42.58%] [G loss: 0.820753, mse: 0.055793]\n",
      "32 [D loss: 0.754136, acc: 19.14%] [G loss: 0.275933, mse: 0.109153]\n",
      "nan value in training set\n",
      "34 [D loss: 0.670796, acc: 25.78%] [G loss: 0.216888, mse: 0.116328]\n",
      "nan value in training set\n",
      "36 [D loss: 0.521694, acc: 41.02%] [G loss: 0.756779, mse: 0.118013]\n",
      "37 [D loss: 0.452708, acc: 49.22%] [G loss: 0.581744, mse: 0.119468]\n",
      "38 [D loss: 0.385294, acc: 58.59%] [G loss: 0.543369, mse: 0.142560]\n",
      "39 [D loss: 0.363776, acc: 60.94%] [G loss: 0.542833, mse: 0.138623]\n",
      "40 [D loss: 0.305839, acc: 65.23%] [G loss: 0.779304, mse: 0.124735]\n",
      "nan value in training set\n",
      "42 [D loss: 0.212235, acc: 71.09%] [G loss: 0.909206, mse: 0.106359]\n",
      "nan value in training set\n",
      "nan value in training set\n",
      "45 [D loss: 0.375477, acc: 57.42%] [G loss: 0.731862, mse: 0.109152]\n",
      "46 [D loss: 0.313961, acc: 61.72%] [G loss: 0.945624, mse: 0.125111]\n",
      "nan value in training set\n",
      "48 [D loss: 0.249072, acc: 69.53%] [G loss: 0.781671, mse: 0.127043]\n",
      "49 [D loss: 0.195524, acc: 75.78%] [G loss: 0.953220, mse: 0.121027]\n",
      "50 [D loss: 0.117749, acc: 84.77%] [G loss: 1.010086, mse: 0.127221]\n",
      "nan value in training set\n",
      "52 [D loss: 0.105680, acc: 86.72%] [G loss: 1.115716, mse: 0.123759]\n",
      "53 [D loss: 0.044002, acc: 94.14%] [G loss: 1.092462, mse: 0.126988]\n",
      "54 [D loss: 0.008653, acc: 99.22%] [G loss: 1.101139, mse: 0.131481]\n",
      "55 [D loss: 0.000944, acc: 100.00%] [G loss: 1.071996, mse: 0.115633]\n",
      "56 [D loss: 0.012252, acc: 98.44%] [G loss: 1.019933, mse: 0.109076]\n",
      "57 [D loss: 0.034094, acc: 94.92%] [G loss: 1.016069, mse: 0.093700]\n",
      "58 [D loss: 0.023731, acc: 98.05%] [G loss: 0.963002, mse: 0.080161]\n",
      "59 [D loss: 0.012830, acc: 98.83%] [G loss: 0.884918, mse: 0.066172]\n",
      "nan value in training set\n",
      "61 [D loss: 0.008130, acc: 99.22%] [G loss: 0.870382, mse: 0.070302]\n",
      "nan value in training set\n",
      "nan value in training set\n",
      "nan value in training set\n",
      "65 [D loss: 0.017732, acc: 98.83%] [G loss: 0.966053, mse: 0.079854]\n",
      "66 [D loss: 0.000163, acc: 100.00%] [G loss: 1.037895, mse: 0.079834]\n",
      "67 [D loss: 0.005260, acc: 99.22%] [G loss: 0.973947, mse: 0.074992]\n",
      "nan value in training set\n",
      "69 [D loss: 0.001442, acc: 100.00%] [G loss: 0.905266, mse: 0.080319]\n",
      "70 [D loss: 0.010281, acc: 99.22%] [G loss: 1.016706, mse: 0.083268]\n",
      "71 [D loss: 0.000212, acc: 100.00%] [G loss: 1.034138, mse: 0.074746]\n",
      "72 [D loss: 0.002253, acc: 99.61%] [G loss: 0.979862, mse: 0.076687]\n",
      "73 [D loss: 0.004429, acc: 99.61%] [G loss: 0.867530, mse: 0.074988]\n",
      "nan value in training set\n",
      "75 [D loss: 0.007956, acc: 100.00%] [G loss: 1.001087, mse: 0.068944]\n",
      "76 [D loss: 0.000959, acc: 100.00%] [G loss: 1.033746, mse: 0.069210]\n",
      "77 [D loss: 0.000817, acc: 100.00%] [G loss: 1.001660, mse: 0.067228]\n",
      "nan value in training set\n",
      "79 [D loss: 0.001199, acc: 100.00%] [G loss: 0.933266, mse: 0.068367]\n",
      "80 [D loss: 0.001111, acc: 100.00%] [G loss: 0.814388, mse: 0.068618]\n",
      "81 [D loss: 0.002712, acc: 100.00%] [G loss: 0.802073, mse: 0.071828]\n",
      "82 [D loss: 0.012919, acc: 99.61%] [G loss: 1.017172, mse: 0.071254]\n",
      "83 [D loss: 0.000703, acc: 100.00%] [G loss: 1.064101, mse: 0.077234]\n",
      "nan value in training set\n",
      "85 [D loss: 0.002924, acc: 100.00%] [G loss: 1.059332, mse: 0.077368]\n",
      "86 [D loss: 0.006471, acc: 99.61%] [G loss: 1.004049, mse: 0.079481]\n",
      "87 [D loss: 0.001357, acc: 100.00%] [G loss: 0.856116, mse: 0.069771]\n",
      "88 [D loss: 0.016400, acc: 98.05%] [G loss: 1.019089, mse: 0.069528]\n",
      "89 [D loss: 0.000239, acc: 100.00%] [G loss: 1.061235, mse: 0.075542]\n",
      "90 [D loss: 0.001626, acc: 99.61%] [G loss: 1.054841, mse: 0.075071]\n",
      "91 [D loss: 0.000471, acc: 100.00%] [G loss: 1.033672, mse: 0.073098]\n",
      "92 [D loss: 0.002643, acc: 99.61%] [G loss: 0.979266, mse: 0.062174]\n",
      "93 [D loss: 0.000393, acc: 100.00%] [G loss: 0.865406, mse: 0.065752]\n",
      "nan value in training set\n",
      "95 [D loss: 0.003964, acc: 100.00%] [G loss: 0.946014, mse: 0.069875]\n",
      "96 [D loss: 0.000357, acc: 100.00%] [G loss: 0.910080, mse: 0.068169]\n",
      "nan value in training set\n",
      "98 [D loss: 0.004395, acc: 100.00%] [G loss: 0.839136, mse: 0.067432]\n",
      "99 [D loss: 0.002636, acc: 100.00%] [G loss: 0.800019, mse: 0.079133]\n",
      "100 [D loss: 0.001737, acc: 100.00%] [G loss: 0.812410, mse: 0.079495]\n",
      "101 [D loss: 0.005223, acc: 99.61%] [G loss: 0.956799, mse: 0.095235]\n",
      "102 [D loss: 0.002442, acc: 100.00%] [G loss: 0.961473, mse: 0.088429]\n",
      "103 [D loss: 0.000545, acc: 100.00%] [G loss: 0.920749, mse: 0.080368]\n",
      "104 [D loss: 0.000983, acc: 100.00%] [G loss: 0.837191, mse: 0.079131]\n",
      "105 [D loss: 0.000685, acc: 100.00%] [G loss: 0.863087, mse: 0.079149]\n",
      "106 [D loss: 0.011692, acc: 99.22%] [G loss: 0.970649, mse: 0.075332]\n",
      "107 [D loss: 0.000152, acc: 100.00%] [G loss: 1.019175, mse: 0.076615]\n",
      "108 [D loss: 0.002549, acc: 100.00%] [G loss: 0.876419, mse: 0.078161]\n"
     ]
    }
   ],
   "source": [
    "combined = Model(masked_img , [gen_missing, valid])\n",
    "combined.compile(loss='mse',optimizer=optimizer)\n",
    "train(epochs=3000, batch_size=128, sample_interval=10)\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
